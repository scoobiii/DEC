Hello dear friends,

The new role of 'prompt engineer' is in fashion.

Let's go.

(GPT speaking) Both GPT (Generative Pre-trained Transformer), which is the underlying model of ChatGPT, and BERT (Bidirectional Encoder Representations from Transformers), which is the underlying model of BARD (Base Model for Answer Retrieval in Dialogues), are examples of state-of-the-art language models based on deep learning. These models have impressive accuracy and are capable of performing natural language tasks with great precision, surpassing many previous approaches.

However, it is important to note that, although these models are highly accurate and powerful, they are not infallible and can make mistakes. The accuracy of a model is the measure of its ability to correctly predict outcomes relative to test data. However, even with high accuracy on test data, the model may not be able to generalize perfectly to new data, which can lead to errors in real-world situations.

Overfitting is a phenomenon that occurs when a model becomes excessively specialized on training data and loses the ability to generalize to new data. This can happen when the model is too complex relative to the size of the training dataset, and it "memorizes" examples instead of learning broader patterns that would be applicable to new data. In this case, the model may have 100% accuracy on the training data, but its performance will be much worse on new unseen data.

Therefore, it is essential to find a balance when training these models, using techniques such as proper data splitting into training and test sets, regularization, and hyperparameter tuning to avoid overfitting and ensure they can generalize well to new data and provide useful and accurate responses in real-world scenarios.

Now, I, "Ask ChatGPT and BARD, 'what happened with Google Cloud on July 19, 2022, in London after the temperature reached 40°C for the first time in history?'"

The first one stuck the boards, the second one could partially respond, but doesn't know how to resolve... It wasn't trained.

The fact is that the City lost, again, its position as the global financial capital, either due to financial losses, estimates above US$500MM, or immeasurable value due to reputation loss.
After all, the over $20 trillion managed by companies like BlackRock, Vanguard, Nimb, World Bank, and other institutions are virtual, not grounded in the real world entirely, and reside in the cloud or worse, in data centers without external redundancy.

Years	Events
2010	DeepMind is founded by Demis Hassabis, Shane Legg, and Mustafa Suleyman in London, UK.
2013	DeepMind gains prominence by winning competitions of classic arcade games using deep reinforcement learning, including games like Pong and Space Invaders.
2014	The company develops the DeepMind system that combines convolutional neural networks with deep reinforcement learning and outperforms human players in various Atari games.
2015	DeepMind develops AlphaGo, an artificial intelligence program that defeats the world Go champion, Lee Sedol, in a series of historic games.
2016	AlphaGo defeats the world Go champion, Ke Jie, and is considered a significant milestone in the advancement of artificial intelligence.
2017	DeepMind develops AlphaZero, which, using only reinforcement learning and self-play, defeats the AlphaGo program, becoming an expert in Go, chess, and shogi.
2018	DeepMind launches AlphaStar, an artificial intelligence program that defeats professional players in StarCraft II, a complex real-time strategy game.
2019	DeepMind announces AlphaFold, a system that uses deep learning to predict protein structures with high accuracy, significantly advancing structural biology.
2020	DeepMind achieves a new milestone by developing AlphaFold 2, which wins the Critical Assessment of Structure Prediction (CASP) competition and outperforms other methods for predicting protein structures.
2021	DeepMind continues to work in various areas of artificial intelligence, expanding its research and collaborations across different industries.

Milestones:

2016: Google announces a 40% reduction in electricity consumption in its data centers since 2010.
2018: Google announces saving $1 billion in energy costs since 2010 by raising the set point of its data centers to 27°C, adopted by Ashrae in the USA and replicated worldwide…
2020: Google announces adopting the global set point policy of 27°C for all its data centers.
2021: Google discloses an increase in the energy bill for its data centers due to the adoption of the global set point policy of 27°C.
2022: Google's data centers in London go offline for several hours due to the increase in external temperature.
2023: Google announces working on implementing measures to improve the resilience of its data centers.

The top 10 demands by sector, industry, client, and application for GCP data centers in London:

Sector	Industry	Client	Application
Technology	Internet	Google	Research and development
Finance	Banks	JPMorgan Chase	Online banking services
Retail	E-commerce	Amazon	Electronic commerce
Health	Hospitals	Mayo Clinic	Online health services
Government	Defense	US Department of Defense	Artificial intelligence services
Education	Universities	Stanford University	Scientific research
Manufacturing	Automotive	Ford Motor Company	Digital manufacturing
Logistics	Transport	UPS	Package tracking
Telecommunications	Mobile	Verizon	5G network services

What does this have to do with Roxinho Nubank?

In 2020, Big Blue halted the Brazilian financial internet. Their servers, with a long tradition in the financial sector, operating in an outsourced campus,

They stopped because there was a lack of chilled water in the cooling systems?
No, in fact, and following the example of GCP, there was no external APP redundancy, meaning that the data center, which refers to the data processing center, lacked networks, and thus, cooling and energy redundancy.
To have true redundancy, DataCenters will need to change names to DataCloud.

Fail reports should not associate or attribute redundancy failures to events or points outside the OSI layer, where the physical elements are routers, servers, networks, but NEVER utilities - energy and cooling. When they do, they show that they do not have external APP redundancy to the DC

 or 'campus.'

Stay away from these service providers.

To operate cooling and engineering systems, 100% accuracy is necessary, and deep learning has it, provided that tensors are fed with other parameters, during training, such as Performance.

In other words, it is easy to reduce energy costs by raising the set point of air conditioning, one minute of energy saving, a drop in performance in the next, and increased energy and costs in the following minute because server performance, a heat machine no different from a coal locomotive from the time of the industrial revolution, is inversely proportional to temperature.

Deep Learning, to be useful, must respect the new and old laws of physics.

Magazine Luiza is already selling used servers from data centers, with a project life management (plm) of 4 years.
Google and all big clouds buy a ProLiant 380DL 256 GB RAM 1TB HD for $10k, use it for four years, get a 20% ROI, sell it for $1k to be resold as obsolete technology at Magalu or be buried in some landfill.

Although these same equipment still has life left, underutilized, it needs to be reevaluated since a prompt already consumes 5 minutes of a shower.

Ask me how.
